---
head:
  - - script
    - rel: preload
      href: https://embed.reddit.com/widgets.js
---

# 2024-4-17

## 回顾

1. 整理和录入aicode1.3.0的需求和任务
2. 在vip群宣传即将推出的inline chat功能，收集种子用户的想法
3. 协助运维落地【在运维层面编写脚本，获取请求的ip列表，并找出存在异常行为的ip，如不停的调用补全接口】 今天已完成
4. 研究和分析 llama.cpp cache 和 llm cache
5. 阅读taby-agent最新release代码

### 研究和分析 llmcpp cache 和 llm cache

llmcpp cache的事情，我查阅了网上资料，从下面reddit 帖子看：


<blockquote class="reddit-embed-bq" style="height:316px" data-embed-height="316"><a href="https://www.reddit.com/r/LocalLLaMA/comments/18xipjx/experiences_with_caching_in_llamacpp/">Experiences with Caching in llama.cpp</a><br> by<a href="https://www.reddit.com/user/Frequent_Valuable_47/">u/Frequent_Valuable_47</a> in<a href="https://www.reddit.com/r/LocalLLaMA/">LocalLLaMA</a></blockquote>

<script setup>
const script = document.createElement('script');
script.src = 'https://embed.reddit.com/widgets.js';
script.async = true;
document.head.appendChild(script);
</script>

展示不出来内容的话，直接点击地址：https://www.reddit.com/r/LocalLLaMA/comments/18xipjx/experiences_with_caching_in_llamacpp/?utm_source=embedv2&utm_medium=post_embed&utm_content=action_bar&embed_host_url=http://localhost:5173/think/day/2024-4-17.html


需要对llama.cpp 配置 cache_prompt 为 true ，cache_prompt 在官方github上地址如下：

https://github.com/ggerganov/llama.cpp/tree/master/examples/server#api-endpoints

## 回家

- 总结今日
- 用 bookmanize chrome 插件继续整理我的书签，分分类，该删的删，每天整理 20个

## 今日好文

1. 《图解大模型推理优化之KV Cache 》https://mp.weixin.qq.com/s/7Fm8LbUN9jQ2HqxPbUU7UQ

## 今日成长

[满分5颗星]

成长指数：:star::star::star::star::star:

- 学习和研究了 llm cache 和 llmcpp cache 的技术实现

## 明日计划

1. 输出缓存的方案文档
2. 阅读taby-agent最新release代码和内网代码
